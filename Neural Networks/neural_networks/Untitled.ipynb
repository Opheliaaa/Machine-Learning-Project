{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f39ff7443d8d>, line 118)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f39ff7443d8d>\"\u001b[0;36m, line \u001b[0;32m118\u001b[0m\n\u001b[0;31m    W = # initialize weights using self.init_weights\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from neural_networks.activations import initialize_activation\n",
    "from neural_networks.weights import initialize_weights\n",
    "from neural_networks.utils.convolution import im2col, col2im, pad2d\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    \"\"\"Abstract class defining the `Layer` interface.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "\n",
    "        self.n_in = None\n",
    "        self.n_out = None\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def clear_gradients(self) -> None:\n",
    "        self.cache = OrderedDict({a: [] for a, b in self.cache.items()})\n",
    "        self.gradients = OrderedDict(\n",
    "            {a: np.zeros_like(b) for a, b in self.gradients.items()}\n",
    "        )\n",
    "\n",
    "    def forward_with_param(\n",
    "        self, param_name: str, X: np.ndarray,\n",
    "    ) -> Callable[[np.ndarray], np.ndarray]:\n",
    "        \"\"\"Call the `forward` method but with `param_name` as the variable with\n",
    "        value `param_val`, and keep `X` fixed.\n",
    "        \"\"\"\n",
    "\n",
    "        def inner_forward(param_val: np.ndarray) -> np.ndarray:\n",
    "            self.parameters[param_name] = param_val\n",
    "            return self.forward(X)\n",
    "\n",
    "        return inner_forward\n",
    "\n",
    "    def _get_parameters(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.parameters.items()]\n",
    "\n",
    "    def _get_cache(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.cache.items()]\n",
    "\n",
    "    def _get_gradients(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.gradients.items()]\n",
    "\n",
    "\n",
    "def initialize_layer(\n",
    "    name: str,\n",
    "    activation: str = None,\n",
    "    weight_init: str = None,\n",
    "    n_out: int = None,\n",
    "    kernel_shape: Tuple[int] = None,\n",
    "    stride: int = None,\n",
    "    pad: int = None,\n",
    "    mode: str = None,\n",
    "    keep_dim: str = \"first\",\n",
    ") -> Layer:\n",
    "    \"\"\"Factory function for layers.\"\"\"\n",
    "    if name == \"fully_connected\":\n",
    "        return FullyConnected(\n",
    "            n_out=n_out, activation=activation, weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    elif name == \"elman\":\n",
    "        return Elman(n_out=n_out, activation=activation, weight_init=weight_init,)\n",
    "\n",
    "    elif name == \"conv2d\":\n",
    "        return Conv2D(\n",
    "            n_out=n_out,\n",
    "            activation=activation,\n",
    "            kernel_shape=kernel_shape,\n",
    "            stride=stride,\n",
    "            pad=pad,\n",
    "            weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Layer type {} is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = # initialize weights using self.init_weights\n",
    "        b = # initialize biases to zeros\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache: OrderedDict = # what do you need cache for backprop?\n",
    "        self.gradients: OrderedDict = # initialize parameter gradients to zeros\n",
    "                                      # MUST HAVE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform an affine transformation and activation\n",
    "\n",
    "        # store information necessary for backprop in `self.cache`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # unpack the cache\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Elman(Layer):\n",
    "    \"\"\"Elman recurrent layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        activation: str = \"tanh\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters.\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = # initialize weights using self.init_weights\n",
    "        U = # initialize weights using self.init_weights\n",
    "        b = # initialize biases to zeros\n",
    "\n",
    "        # initialize the cache, save the parameters, initialize gradients\n",
    "        self.parameters: OrderedDict = OrderedDict({\"W\": W, \"U\": U, \"b\": b})\n",
    "        self.gradients: OrderedDict = ...\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def _init_cache(self, X_shape: Tuple[int]) -> None:\n",
    "        \"\"\"Initialize the layer cache. This contains useful information for\n",
    "        backprop, crucially containing the hidden states.\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        s0 = # the first hidden state\n",
    "        self.cache = OrderedDict({\"s\": [s0], ...})  # THIS IS INCOMPLETE\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward_step(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute a single recurrent forward step.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        `self.cache[\"s\"]` is a list storing all previous hidden states.\n",
    "        The forward step is computed as:\n",
    "            s_t+1 = fn(W X + U s_t + b)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a recurrent forward step\n",
    "\n",
    "        # store information necessary for backprop in `self.cache`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass for `t` time steps. This should involve using\n",
    "        forward_step repeatedly, possibly in a loop. This should be fairly simple\n",
    "        since `forward_step` is doing most of the heavy lifting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix containing inputs for `t` time steps\n",
    "           shape (batch_size, input_dim, t)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the final output/hidden state\n",
    "        shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape[:2])\n",
    "\n",
    "        self._init_cache(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        Y = []\n",
    "        # perform `t` forward passes through time and return the last\n",
    "        # hidden/output state\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return Y[-1]\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"Backward pass for recurrent layer. Compute the gradient for all the\n",
    "        layer parameters as well as every input at every time step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of numpy arrays of shape (batch_size, input_dim) of length `t`\n",
    "        containing the derivative of the loss with respect to the input at each\n",
    "        time step\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # unpack the cache\n",
    "\n",
    "        dLdX = []\n",
    "\n",
    "        # perform backpropagation through time, storing the gradient of the loss\n",
    "        # w.r.t. each time step in `dLdX`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dLdX\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    \"\"\"Convolutional layer for inputs with 2 spatial dimensions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters.\"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # initialize weights, biases, the cache, and gradients\n",
    "\n",
    "        self.parameters: OrderedDict = ...\n",
    "        self.cache: OrderedDict = ...\n",
    "        self.gradients: OrderedDict = ...\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "\n",
    "        # cache any values required for backprop\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def forward_faster(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        This implementation uses `im2col` which allows us to use fast general\n",
    "        matrix multiply (GEMM) routines implemented by numpy. This is still\n",
    "        rather slow compared to GPU acceleration, but still LEAGUES faster than\n",
    "        the nested loop in the naive implementation.\n",
    "\n",
    "        DO NOT ALTER THIS METHOD.\n",
    "\n",
    "        You will write your naive implementation in forward().\n",
    "        We will use forward_faster() to check your method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        X_col, p = im2col(X, kernel_shape, self.stride, self.pad)\n",
    "\n",
    "        out_rows = int((in_rows + p[0] + p[1] - kernel_height) / self.stride + 1)\n",
    "        out_cols = int((in_cols + p[2] + p[3] - kernel_width) / self.stride + 1)\n",
    "\n",
    "        W_col = W.transpose(3, 2, 0, 1).reshape(out_channels, -1)\n",
    "\n",
    "        Z = (\n",
    "            (W_col @ X_col)\n",
    "            .reshape(out_channels, out_rows, out_cols, n_examples)\n",
    "            .transpose(3, 1, 2, 0)\n",
    "        )\n",
    "        Z += b\n",
    "        out = self.activation(Z)\n",
    "\n",
    "        self.cache[\"Z\"] = Z\n",
    "        self.cache[\"X\"] = X\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward_faster(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        This uses im2col, so it is considerably faster than the naive implementation\n",
    "        even on a CPU.\n",
    "\n",
    "        DO NOT ALTER THIS METHOD.\n",
    "\n",
    "        You will write your naive implementation in backward().\n",
    "        We will use backward_faster() to check your method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        X = self.cache[\"X\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        dZ = self.activation.backward(Z, dLdY)\n",
    "\n",
    "        dZ_col = dZ.transpose(3, 1, 2, 0).reshape(dLdY.shape[-1], -1)\n",
    "        X_col, p = im2col(X, kernel_shape, self.stride, self.pad)\n",
    "        W_col = W.transpose(3, 2, 0, 1).reshape(out_channels, -1).T\n",
    "\n",
    "        dW = (\n",
    "            (dZ_col @ X_col.T)\n",
    "            .reshape(out_channels, in_channels, kernel_height, kernel_width)\n",
    "            .transpose(2, 3, 1, 0)\n",
    "        )\n",
    "        dB = dZ_col.sum(axis=1).reshape(1, -1)\n",
    "\n",
    "        dX_col = W_col @ dZ_col\n",
    "        dX = col2im(dX_col, X, W.shape, self.stride, p).transpose(0, 2, 3, 1)\n",
    "\n",
    "        self.gradients[\"W\"] = dW\n",
    "        self.gradients[\"b\"] = dB\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d7320417d403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d7320417d403>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "output=[]\n",
    "for i in range(a.shape[0]):\n",
    "    row=[z if z>=0 else 0 for z in a[i]]\n",
    "    output.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
