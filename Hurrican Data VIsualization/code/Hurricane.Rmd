```{r}

library(maps); library(gridExtra); library(ggmap); library(ggplot2); library(plyr); library(raster); library(RColorBrewer); library(maptools); library(rgeos); library(sp); library(rgdal); library(geosphere); library(stringi); library(data.table);library(plyr)

```

```{r}

#Parsing Data for Best Track Line from NHC
best_track = fread("https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2017-050118.txt", blank.lines.skip = TRUE, header = FALSE, fill = TRUE)
#Create variable for recording category of hurricanes
best_track$category = vector()
best_track$category[best_track$V7 > 64] = 1
best_track$category[best_track$V7 > 83] = 2
best_track$category[best_track$V7 > 96] = 3
best_track$category[best_track$V7 > 113] = 4
best_track$category[best_track$V7 > 137] = 5
best_track$category[is.na(best_track$category)] = 0 

```


```{r}
#Parsing Data for Watchzone from NHC
cities = read.table("../data/wwpts_us.txt", fill = TRUE, col.names = rep("V", 10))
```

```{r}
#now clean up data to look like what we want
index_cities = grep("=", cities[, 2])
number_index_list = gregexpr("[0-9]", cities[,2])
number_index = which(sapply(number_index_list, "[[", 1) == 1)
reps = diff(number_index) - 1
code = cities[number_index, 7]
date = paste0(cities[number_index, 1], "/", cities[number_index, 2], "/", cities[number_index, 3])
name = cities[number_index, 5]
hour = cities[number_index, 4]
```


```{r}
#get rid of improper coding
city = paste0(cities[index_cities, 2], cities[index_cities, 3], cities[index_cities, 4], cities[index_cities, 5], cities[index_cities, 6], cities[index_cities, 7], cities[index_cities, 8], cities[index_cities, 9])
city[city == "=PuertoRico=Vieques=Culebra="] = "=PuertoRico=Culebra="
city[city == "=LakeOkeechobee="] = "=LakeOkeechobee=LakeOkeechobee="
lst = strsplit(city, "=")
city_origin = sapply(lst, "[[", 2)
city_destination = sapply(lst, "[[", 3)
city_warning = (cities[index_cities, 1]) 
name = rep(name[-length(name)], reps)
code = rep(code[-length(code)], reps)
date = rep(date[-length(date)], reps)
hour = rep(hour[-length(hour)], reps)
```




```{r}
#Create final dataframe for the watchzone data
final_df = data.frame(code = code, name = name, date = date, hour = hour, city_origin = city_origin, city_destination = city_destination, warning = city_warning)
```

```{r}
#Add geocoded variables
path = "../data/geocode_raw.csv"
geocoded_df = read.csv(path, header = TRUE)

final_df$origin_lat = geocoded_df$origin_lat
final_df$origin_long = geocoded_df$origin_long

final_df$destin_lat = geocoded_df$destin_lat
final_df$destin_long = geocoded_df$destin_long

corrected = function(city, lat, long) {
  city = tolower(city)
  final_df$origin_lat[tolower(final_df$city_origin) == city] = lat
  final_df$origin_long[tolower(final_df$city_origin) == city] = long
  final_df$destin_lat[tolower(final_df$city_destination) == city] = lat
  final_df$destin_long[tolower(final_df$city_destination) == city] = long
  return(final_df)
}
```





```{r}

#Corrected
final_df = corrected("BAYPORT", 28.54306, -82.64401)
final_df = corrected("FLAMINGO", 25.14179, -80.92534)
final_df = corrected("VENICE", 27.09977, -82.45426)
final_df = corrected("FREEPORT", 30.51572, -86.13620)
final_df = corrected("CAMERON", 29.79770, -93.32520)
final_df = corrected("MOBILE", 30.69540, -88.03990)
final_df = corrected("VERMILIONBAY", 29.71965, -91.97623)
final_df = corrected("KEYWEST", 25.74590, -80.55500)
final_df = corrected("DRYTORTUGAS", 26.14200, -81.79480) #Just the island is weird
final_df = corrected("BAFFINBAY", 27.26700, -97.58210)
final_df = corrected("SARGENT", 28.83530, -95.66470)
final_df = corrected("OCEANREEF", 25.31170, -80.27940)
final_df = corrected("MISSISSIPPIRIVER", 29.16690, -89.25030)
final_df = corrected("ROCKPORT", 28.02060, -97.05440)
final_df = corrected("ROCKPORT", 28.02060, -97.05440)
final_df = corrected("SABINERIVER", 30.10236, -93.74420)
final_df = corrected("CAPEANN", 42.61590, -70.66200)
final_df = corrected("CAPELOOKOUT", 34.60530, -76.53670)
final_df = corrected("BRUNSWICK", 31.15000, -81.49150)
final_df = corrected("SAINTMARKS", 30.16100, -84.20630)
final_df = corrected("FENWICKISLAND", 38.46230, -75.05130)
final_df = corrected("SANDYHOOK", 41.43330, -73.98850)
final_df = corrected("CHATHAM", 41.68210, -69.95980)
final_df = corrected("STUART", 27.19750, -80.25280)
final_df = corrected("NC/VABORDER", 36.51880, -75.91990)
final_df = corrected("OregonInletI/PSAS", 36.51880, -75.91990)
final_df = corrected("NC/VABorderI/PSAS", 36.51880, -75.91990)
final_df = corrected("OceanReefI/FB", 25.31590, -80.27990)
final_df = corrected("FlaglerBeachI/LO", 29.47500, -81.12700)
final_df = corrected("StAugustineI/LO", 29.47500, -81.12700)
final_df = corrected("FloridaCity", 27.66480, -81.51580)
final_df = corrected("Englewood", 26.96200, -82.35260)

c_df = final_df[(final_df$warning == 5 | final_df$warning == 6), ]
```


```{r}
#Helper clean up data file functions
#First two functions for regular expression matching
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

substrLeft = function(x, n){
  substr(x, 1, n)
}
```

```{r}
#This function is to extract the relevant best track line data for any specific hurricane name we desire
best_track_line = function(name, year, idf) {
  name = tolower(name)
  index = which(tolower(best_track$V2) == name)
  start = index[which(substrRight(best_track$V1[index], 4) == year)]
  hurricane = grep("[A-Z]", best_track$V2)
  end = hurricane[which(hurricane == start) + 1]
  in_df = best_track[start:(end-1), ]
  in_df = in_df[-1, ]
  date = as.character(idf$date)
  in_date = substr(date, nchar(date) - 6, nchar(date) -5)
  dex = which(substrRight(in_df$V1, 2) == in_date)
  in_df_1 = in_df[1:dex[length(dex)], ]
  x = substrLeft(in_df$V6, 4); y = substrLeft(in_df$V5, 4); x = gsub("[^0-9\\.]", "", x); y = gsub("[^0-9\\.]", "", y) 
  x = -as.numeric(x); y = as.numeric(y)
  bt_df = data.frame(x = x, y = y, category = in_df$category)
  return(bt_df)
}
```



```{r}
#This function is to clean up the hurricane dataset the way we want
cleanup_df = function(name, year) {
  name = tolower(name)
  index = which(tolower(c_df$name) == name)
  l = strsplit(as.character(c_df$date[index]), "/")
  final_index = which(sapply(l, "[[", 3) == year)
  final_index = index[final_index]
  df = c_df[final_index, ]
  a = unique(df$date)
  lst = list()
  for (i in 1:length(a)) {
    l = which(df$date == a[i])
    d = df[l, ]
    l = unique(d$hour)
    for(j in 1:length(l)) {
      aa = which(d$hour == l[j])
      if (length(aa) == 1) {
        d[aa, ] = d[aa, ]
      } 
      if (length(aa) != 1) {
        dist = (d$origin_lat[aa] - d$destin_lat[aa])^2 + (d$origin_long[aa] - d$destin_long[aa])^2
        chosen = which.max(dist)
        index = which(aa != aa[chosen])
        d = d[-c(aa[index]), ]
      }
    }
    lst[[i]] = d
  }
  df = ldply(lst, data.frame)
  return(df)
}

#us <- getData('GADM', country = 'USA', level = 1)
#a = makegrid(us, n = 1000000)
#grid = SpatialPoints(a, proj4string = CRS(proj4string(us)))
#grid = grid[us, ]
#grid = as.data.frame(grid)
```



```{r}
#Database for all hurricanes - Extracting all the hurricane names and dates from the NHC data
#Hurricane names:
hurricane_names = unique(c_df$name)
```

```{r}
#Extract dates
extract_date = function(hurricane, data = c_df) {
  index = which(c_df[, 2] == hurricane)
  index = index[1]
  out = c_df[, 3][index]
  a = substrRight(x = as.character(out), n = 4)
  return(a)
}
dates = vector(length = 0)
for (i in 1:length(hurricane_names)) {
  dates[i] = extract_date(hurricane = hurricane_names[i], data = c_df)
}

hurricane_names = as.character(hurricane_names)

get_landfall_date = function(name, year) {
  name = tolower(name)
  index = which(tolower(best_track$V2) == name)
  start = index[which(substrRight(best_track$V1[index], 4) == year)]
  hurricane = grep("[A-Z]", best_track$V2)
  end = hurricane[which(hurricane == start) + 1]
  in_df = best_track[start:(end-1), ]
  in_df = in_df[-1, ]
  in_date = (in_df$V1[which(in_df$V3 == "L")])
  in_date = as.Date(in_date, "%Y%m%d")
  return(in_date)
}

```



```{r}
#Missing hurricanes
#i = 77 take out because eighteen is not here
hurricane_names = hurricane_names[-77]
dates = dates[-77]
hurricane_names = hurricane_names[-96]
dates = dates[-96]
```

```{r}
#Grid generation (ignore)

# load some spatial data. Administrative Boundary
us <- getData('GADM', country = 'USA', level = 1)
a = makegrid(us, n = 1000000)
#grid = SpatialPoints(a, proj4string = CRS(proj4string(us)))
#grid = grid[us, ]
#grid = as.data.frame(grid)
#save(grid, file = "grid.rda")
load("~/Desktop/berkeley/urap/hurricane/data/grid.rda")
```




```{r}
watch_coord_distance_updated = function(watchlist_df, coords) {
  watchlist_df$dist = distHaversine(coords, watchlist_df[, c("long", "lat")])/1609
  index = which.min(watchlist_df$dist)
  dis = watchlist_df$dist[index]
  return(watchlist_df[index, ])
}

intersection_updated = function(bt_df, i_start, i_end, dist = 50, m = 1) {
  line_df = bt_df[, -3]
  a = i_start - dist 
  b = i_end + dist
  if (a < 0) {
    a = 1
  }
  if (b > 7243) {
    b = 7243
  }
  shape_df = all_state[(a):(b), ]
  shape_df = shape_df[ , ]
  d = list()
  for (i in 1:length(shape_df$long)) {
    d[[i]] = distHaversine(line_df, shape_df[i, c("long", "lat")])/1609
  }
  test = sapply(d, min)
  a = which(sort(test)[m] == test)
  intersection = c(shape_df[a, ])
  ind = which(d[[a]] == test[a])
  a = line_df[ind, ]
  data_a = data.frame(x = a$x, y = a$y, long = intersection$long, lat = intersection$lat)
  return(data_a)
}
```

```{r}
all_state = map_data("usa")
```

##read the landfalldates.csv to get landfall dates/category
```{r}
landfall=read.csv("../data/landfalldates.csv",header = TRUE)
landfall=landfall[,-1]
landfall$landfalldate=as.Date(landfall$landfalldate)
```



##output csv files needed to draw the heatmap
```{r}
get_csv = function(name, year, day = 3, dist = 50, maxdist = 75, m = 1, id = 1) {
  df = cleanup_df(name, year); df$dist = distHaversine(df[, c("origin_long", "origin_lat")], df[, c("destin_long", "destin_lat")])/1609;
  u = unique(df$date); index = list()
  for (i in 1:length(u)) {
    j = which(df$date == unique(df$date)[i])
    a = which.max(df[j, ]$dist)
    index[[i]] = df[j, ][a, ]
  }
  i_df = ldply(index, data.frame); n = length(index); i_df$x = c()
  all_state = map_data("usa"); state_map = ggplot() + geom_polygon(data=all_state, aes(x=long, y=lat, group = group), colour="white", fill="grey80")
  vec = list(length = n); date = unique(i_df$date);
  ind = which(landfall$hurricanes == paste0(toupper(name),id))
  i = which(as.Date(i_df$date, format = "%m/%d/%Y") == landfall$landfalldate[ind]) - day
  if (i < 0) {
    stop(print(n))
  }
  x_min = min(c(i_df[i, ]$origin_long, i_df[i, ]$destin_long)); x_max = max(c(i_df[i, ]$origin_long, i_df[i, ]$destin_long)); y_max = max(c(i_df[i, ]$origin_lat, i_df[i, ]$destin_lat)); y_min = min(c(i_df[i, ]$origin_lat, i_df[i, ]$destin_lat))
  x = c(i_df$origin_long[i_df$date == date[i]], i_df$destin_long[i_df$date == date[i]])
  y = c(i_df$origin_lat[i_df$date == date[i]], i_df$destin_lat[i_df$date == date[i]])
  in_df = data.frame(x = x, y = y)
  all_state$distance1 = distHaversine(in_df[1, ], all_state[, c("long", "lat")])/1609
  all_state$distance2 = distHaversine(in_df[2, ], all_state[, c("long", "lat")])/1609
  index_start = which.min(all_state$distance1); index_end = which.min(all_state$distance2)
  i_start = sort(c(index_start, index_end))[1]; 
  i_end = sort(c(index_start, index_end))[2]; 
  bt_df = best_track_line(name = name, year = year, idf = i_df[i, ])
  intersect = intersection_updated(bt_df, i_start = i_start, i_end = i_end, dist = dist, m = m)[1, ]
  grid_1 = grid
  grid_1$distance_track = distHaversine(grid, intersect[, c("long", "lat")])/1609
  new_grid = grid_1[grid_1$distance_track < maxdist, ]
  distance_watch = vector()
  watchzone_ref_long = vector()
  watchzone_ref_lat = vector()
  for (j in 1:nrow(new_grid)) {
    distance_watch[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$dist
    watchzone_ref_long[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$long
    watchzone_ref_lat[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$lat
  }
  new_grid$distance_watch = distance_watch
  num = nrow(new_grid)
  new_grid$origin_lat = rep(i_df[i, "origin_lat"], num)
  new_grid$origin_long = rep(i_df[i, "origin_long"], num)
  new_grid$origin_ref = rep(i_start, num)
  new_grid$destin_lat = rep(i_df[i, "destin_lat"], num)
  new_grid$destin_long = rep(i_df[i, "destin_long"], num)
  new_grid$destin_ref = rep(i_end, num)
  new_grid$date_analyzed = rep(i_df[i, 4], num)
  new_grid$max_category = rep(max(bt_df$category), num)
  new_grid$watchzone_ref_long = watchzone_ref_long
  new_grid$watchzone_ref_lat = watchzone_ref_lat
  new_grid$intersection_long = rep(intersect$long, num)
  new_grid$intersection_lat = rep(intersect$lat, num)
  new_grid$intersect_ref_long = rep(intersect$x, num)
  new_grid$intersect_ref_lat = rep(intersect$y, num)
  colnames(new_grid)[c(1,2)] = c("long", "lat")
  file_name = paste0("~/desktop/berkeley/urap/hurricane/csv/", year, "_", tolower(name), id, "_", day, ".csv")
  write.csv(new_grid,file =file_name)
}
```


## draw heaptmaps for a specific hurricane and save them to local directory
```{r}
get_heat_map_2 = function(name, year, day = 3, dist = 50, maxdist = 200, limit = 5, m = 1, id = 1) {
  df = cleanup_df(name, year); df$dist = distHaversine(df[, c("origin_long", "origin_lat")], df[, c("destin_long", "destin_lat")])/1609;
#  u = unique(df$date); index = list()
#  for (i in 1:length(u)) {
 #   j = which(df$date == unique(df$date)[i])
 #   a = which.max(df[j, ]$dist)
 #   index[[i]] = df[j, ][a, ]
  #}
 # i_df = ldply(index, data.frame); n = length(index); i_df$x = c()
  #vec = list(length = n); #date = unique(i_df$date);
  ind = which(landfall$hurricanes == paste0(toupper(name),id))
  landfalldate = landfall$landfalldate[ind]
  date_interested = landfalldate - day
  if (date_interested < as.Date(df$date[1], format = "%m/%d/%Y")) {
    stop(print("potato"))
  }
  i_df = df%>%filter(as.Date(date, format = "%m/%d/%Y") == date_interested)
  n = nrow(i_df)
  #i = which(as.Date(i_df$date, format = "%m/%d/%Y") == landfall$landfalldate[ind]) - day
 # if (i < 0) {
  #  stop(print("potato"))
 # }
  all_state = map_data("usa"); state_map = ggplot() + geom_polygon(data=all_state, aes(x=long, y=lat, group = group), colour="white", fill="grey80")
  
  for (i in 1:n) {
    
  x_min = min(c(i_df[i, ]$origin_long, i_df[i, ]$destin_long)); x_max = max(c(i_df[i, ]$origin_long, i_df[i, ]$destin_long)); y_max = max(c(i_df[i, ]$origin_lat, i_df[i, ]$destin_lat)); y_min = min(c(i_df[i, ]$origin_lat, i_df[i, ]$destin_lat))
  x = c(i_df$origin_long[i], i_df$destin_long[i])
  y = c(i_df$origin_lat[i], i_df$destin_lat[i])
  in_df = data.frame(x = x, y = y)
  all_state$distance1 = distHaversine(in_df[1, ], all_state[, c("long", "lat")])/1609
  all_state$distance2 = distHaversine(in_df[2, ], all_state[, c("long", "lat")])/1609
  index_start = which.min(all_state$distance1); index_end = which.min(all_state$distance2)
  i_start = sort(c(index_start, index_end))[1]; 
  i_end = sort(c(index_start, index_end))[2]; 
  bt_df = best_track_line(name = name, year = year, idf = i_df[i, ])
  intersect = intersection_updated(bt_df, i_start = i_start, i_end = i_end, dist = dist, m = m)[1, ]
  grid_1 = grid
  grid_1$distance_track = distHaversine(grid, intersect[, c("long", "lat")])/1609
  new_grid = grid_1[grid_1$distance_track < maxdist, ]
  distance_watch = vector()
  watchzone_ref_long = vector()
  watchzone_ref_lat = vector()
  for (j in 1:nrow(new_grid)) {
    distance_watch[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$dist
    watchzone_ref_long[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$long
    watchzone_ref_lat[j] = watch_coord_distance_updated(watchlist_df = all_state[i_start:i_end, ], coords = new_grid[j, 1:2])$lat
  }
  new_grid$distance_watch = distance_watch
  num = nrow(new_grid)
  new_grid$origin_lat = rep(i_df[i, "origin_lat"], num)
  new_grid$origin_long = rep(i_df[i, "origin_long"], num)
  new_grid$origin_ref = rep(i_start, num)
  new_grid$destin_lat = rep(i_df[i, "destin_lat"], num)
  new_grid$destin_long = rep(i_df[i, "destin_long"], num)
  new_grid$destin_ref = rep(i_end, num)
  new_grid$date_analyzed = rep(i_df[i, 4], num)
  new_grid$max_category = rep(max(bt_df$category), num)
  new_grid$watchzone_ref_long = watchzone_ref_long
  new_grid$watchzone_ref_lat = watchzone_ref_lat
  new_grid$intersection_long = rep(intersect$long, num)
  new_grid$intersection_lat = rep(intersect$lat, num)
  new_grid$intersect_ref_long = rep(intersect$x, num)
  new_grid$intersect_ref_lat = rep(intersect$y, num)
  colnames(new_grid)[c(1,2)] = c("long", "lat")
  new_grid$news = new_grid$distance_track - distance_watch
  bound = make_bbox(lat = c(y_min - limit, y_max + limit), lon = c(x_min - limit, x_max + limit))
  g_maps = get_map(location = bound, source = "google", maptype = "terrain")

  vec = ggmap(g_maps) + geom_point(data = new_grid, aes(x = long, y = lat, col = news), size = 2.5, shape = 15, alpha = 0.5) + scale_colour_gradient(low = "red", high = "green") + geom_point(data = i_df[i, ], aes(x = origin_long, y = origin_lat), col = "red") + geom_point(data = as.data.frame(intersect), aes(x = long, y = lat), col = "yellow", size = 2) + geom_point(data = i_df[i, ], aes(x = destin_long, y = destin_lat), col = "red") + geom_path(data = all_state[i_start:i_end, ], aes(x = long, y = lat, group = group), col = "red") + geom_path(aes(x = x, y = y), data = bt_df, col = "blue") + geom_point(data = bt_df, aes(x = x, y = y, shape = factor(category))) + ggtitle(paste0(toupper(name), ": ", day*24-as.integer(as.character(i_df[i, ]$hour))+landfall$hour[landfall$hurricanes==paste0(toupper(name),id)], " hour(s) before ", landfalldate, "|category: ", landfall$category[landfall$hurricanes==paste0(toupper(name),id)]))+
 theme(plot.title = element_text(size=20,vjust=3))
  print(vec)
  file_name = paste0("~/desktop/berkeley/urap/hurricane/output/heatmap/", year, "_", tolower(name), id, "_", day, "_", i, ".png")
  ggsave(file_name,width = 10, height = 14)
  }
}
```
 
```{r}
get_heat_map_2(name='kate', year=1985, day = 0, dist = 50, maxdist = 200, limit = 5, m = 1, id = 1)
```


